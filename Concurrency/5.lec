Что-то про память и CPU

Память в 100 раз медленнее, чем процессор уже сейчас.
Поэтому применяется кэширование. Термины cache hit / cache miss.

В случае cache-miss мы считываем линию (64 байта = 8 машинных слов)
Также кэшей много - L1, L2, L3.
Чем раньше кэш - тем ближе к процессору, тем меньше по объёму, тем дороже. 
Кэш L1 имеет ~32Kb, следующий ~2Mb
L1 разделён на части - для инструкций и для данных.

Что такое кэш вообще - железная хэштаблица.

Интересная аналогия с пивом.

Поймём почему он эффективен.
Основан на двух штуках
* Пространственная локальность  - обращение к ячейки памяти значит, что скорее всего вы обратитесь к соседней ячейке. (про сохранение кэшлинии) 
* Временная локальность - обращение к ячейке значит, что скоро вы снова обратитесь к ячейке.

Зачем нам об этом знать?
Разумно её учитывать, когда вы пишете свой код.
Пример - сравнение итерации по листу и по вектору.

Пример - умножение матриц.
Чтобы матрицы умножались быстрее (1000мс -> 200мс) можно транспонировать матрицу.

Пример - бинарный поиск.
Существует укладка -Какоготочела-, чтобы ускорять бинпоиск.

Оказывается у каждого ядра есть свой кэш, которые вновь связываются в одну систему.

Пример про копии одних и тех же данных на 16 ядрах и то, что ядра видят в каждый момент времени. 

Разделяемая память - абстракция, на самом деле это кэши и память. Это в каком-то смысле распределённая система.

Протокол когерентности - про синхронизацию данных.
Инвариант.
Состояние S - Shared - если в нескольких кэшей ядер. (Кэш не уверен, что у него единственная копия, но знает что значение актуально)
Состояние M - Modified - ядро владеет линией эксклюзивно только что после изменения. (Данные не ещё лежат в памяти)
Состояние I - Invalid. // какое-то дефолтное значение
Состояние E - Exclusive - ядро владеет линией эксклюзивно (гарантированно).
// MESI

Пример - shared mutex.
shared_lock/unlock - для Reader-ов // может быть несколько
lock и shared_lock - секции не пересекаются.

Вообще у кэшей должна быть коммуникация между кэшами.
Чтобы это понять - есть сайт MESI protoctol.
atomic_broadcast - лежит под капотом.

Мораль в этом курсе - Вот эта MESI-коммуникация задаёт нам стоимость нашей синхронизации.
В нашем курсе - зачем это. Постараемся поменять наши структуры, сделав их лучше.

Имеем дефолтный AtomicCounter. 
compare_exchange - всегда делает запись.
Работает 200мс

SharedCounter1.
Сделаем N коунтеров:
Increment = some().increment
Get = all().sum()
Проблема - FalseSharing - наши потоки синхронизировали кэшлинии(и атомики) целиком.

SharedCounter2.
Решим это помещением 
alignas(64) std::atomic<..> …
Или добавим поле char x[64] - костыль.
Работает 50мс

С С++17 есть константы std::hardware_destructive_interference_size // constructive

Реализуем RingBuffer - типа очереди короче, на векторе и двух атомиках.
Получили 1000мс.
Для безопасных реализаций push_back и pop_front требует доступа к обоим атомикам.
Прикол - будем кэшировать tail и head и разделим через alignas наши 4 атомика.
Ускорили в 2 раза.
Прикол - поставить alignas в Values вектора.

Мемори ордеры - сложная тема.
Потребовался acquire/release/relaxed.
Расставив их - ускорим до 150мс.

В нашем ранее написанном SpinLock-е.
Явление - cache ping-pong.
Поскольку exchange гарантированно меняет, то потоки, которые ждут лока, пинг-понгают кэшлинию друг меж другом в состояние Modified.
Решение - внутрь нашего обычного while - while с ожиданием на чтении. 
Ускорение в два раза.

Ещё одна проблема - Thundering Herd. Когда все потоки ждали на чтении, а потом ломанулись делать exchange.

Это всё баловства на самом деле.
Несмотря на ускорение блокировки - взятие мьютекса/лока/etc. Заставляет процессор двигать данные (на запись), таким образом делая всю нашу проделанную незначительно малой. 

mapreduce - двигать не данные, а двигать код. Т.е. исполнять код двух критических секций подряд, на одном ядре, таким образом не двигая данные. 

Написав свой планировщик и свою инфраструктуру.
